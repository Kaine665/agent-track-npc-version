/**
 * ============================================
 * LLM æœåŠ¡å±‚ (LLMService.js)
 * ============================================
 *
 * ã€æ–‡ä»¶èŒè´£ã€‘
 * å°è£… LLM API è°ƒç”¨ï¼Œæ”¯æŒå¤šä¸ªæä¾›å•†ï¼ˆOpenAIã€DeepSeekã€OpenRouterï¼‰
 *
 * ã€ä¸»è¦åŠŸèƒ½ã€‘
 * 1. å¤šæä¾›å•†æ”¯æŒï¼ˆOpenAIã€DeepSeekã€OpenRouterï¼‰
 * 2. Prompt æ„å»ºï¼ˆsystemPrompt + messagesï¼‰
 * 3. é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶ï¼ˆæœ€å¤šé‡è¯• 2 æ¬¡ï¼Œé—´éš” 1 ç§’ï¼‰
 * 4. è¶…æ—¶å¤„ç†ï¼ˆ30 ç§’ï¼‰
 * 5. ç»Ÿä¸€é”™è¯¯æ ¼å¼
 *
 * ã€å·¥ä½œæµç¨‹ã€‘
 * æ¥æ”¶å‚æ•° â†’ é€‰æ‹©æä¾›å•† â†’ æ„å»ºè¯·æ±‚ â†’ è°ƒç”¨ API â†’ é‡è¯•ï¼ˆå¦‚éœ€è¦ï¼‰â†’ è¿”å›ç»“æœ
 *
 * ã€ä¾èµ–ã€‘
 * - config/models.js: è·å–æ¨¡å‹æä¾›å•†ä¿¡æ¯
 * - Node.js å†…ç½® fetchï¼ˆNode.js 18+ï¼‰
 *
 * ã€è¢«è°ä½¿ç”¨ã€‘
 * - services/MessageService.js: è°ƒç”¨ç”Ÿæˆå›å¤
 *
 * ã€é”™è¯¯å¤„ç†ã€‘
 * - API é”™è¯¯ï¼šæŠ›å‡ºåŒ…å«é”™è¯¯ç å’Œæ¶ˆæ¯çš„å¯¹è±¡
 * - è¶…æ—¶é”™è¯¯ï¼šæŠ›å‡º LLM_API_TIMEOUT
 * - ç½‘ç»œé”™è¯¯ï¼šè‡ªåŠ¨é‡è¯•
 *
 * ã€ç¯å¢ƒå˜é‡ã€‘
 * - OPENAI_API_KEY: OpenAI API Keyï¼ˆæ”¯æŒå¤šä¸ªï¼Œç”¨é€—å·åˆ†éš”ï¼škey1,key2,key3ï¼‰
 * - DEEPSEEK_API_KEY: DeepSeek API Keyï¼ˆæ”¯æŒå¤šä¸ªï¼Œç”¨é€—å·åˆ†éš”ï¼škey1,key2,key3ï¼‰
 * - OPENROUTER_API_KEY: OpenRouter API Keyï¼ˆæ”¯æŒå¤šä¸ªï¼Œç”¨é€—å·åˆ†éš”ï¼škey1,key2,key3ï¼‰
 *
 * ã€å¤š API Key æ•…éšœè½¬ç§»ã€‘
 * - æ”¯æŒåœ¨ç¯å¢ƒå˜é‡ä¸­é…ç½®å¤šä¸ª API Keyï¼Œç”¨é€—å·åˆ†éš”
 * - æŒ‰é¡ºåºå°è¯•æ¯ä¸ª API Keyï¼Œå¦‚æœå¤±è´¥ï¼ˆ401/403/429/è¶…æ—¶/ç½‘ç»œé”™è¯¯ï¼‰è‡ªåŠ¨åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ª
 * - å¦‚æœæ‰€æœ‰ API Key éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºæœ€åä¸€ä¸ªé”™è¯¯
 * - ç¤ºä¾‹ï¼šOPENROUTER_API_KEY=key1,key2,key3
 *
 * @author AI Assistant
 * @created 2025-11-20
 * @lastModified 2025-11-20
 */

const { getModelProvider } = require("../config/models");

/**
 * æä¾›å•†é…ç½®
 *
 * ã€åŠŸèƒ½è¯´æ˜ã€‘
 * å®šä¹‰æ¯ä¸ªæä¾›å•†çš„ API ç«¯ç‚¹å’Œé…ç½®
 *
 * ã€API Key è·å–æ–¹å¼ã€‘
 * å½“å‰ï¼šä»ç¯å¢ƒå˜é‡è¯»å–ï¼ˆprocess.env[apiKeyEnv]ï¼‰
 * - OPENAI_API_KEY
 * - DEEPSEEK_API_KEY
 * - OPENROUTER_API_KEY
 *
 * ã€æœªæ¥æ‰©å±•ã€‘
 * å¦‚æœå°†æ¥éœ€è¦æ”¯æŒç”¨æˆ·è‡ªå®šä¹‰ API Keyï¼ˆä¾‹å¦‚åœ¨åˆ›å»º Agent æ—¶æŒ‡å®šï¼‰ï¼Œå¯ä»¥ï¼š
 * 1. åœ¨ models.js ä¸­æ·»åŠ  API Key é…ç½®é€‰é¡¹
 * 2. åœ¨ callLLMAPI() å‡½æ•°ä¸­æ·»åŠ å‚æ•°ï¼šcallLLMAPI(provider, model, systemPrompt, messages, timeout, customApiKey)
 * 3. ä¼˜å…ˆä½¿ç”¨ customApiKeyï¼Œå¦‚æœæœªæä¾›åˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„ API Key
 * 4. ç¤ºä¾‹ï¼š
 *    const apiKey = customApiKey || process.env[config.apiKeyEnv];
 *
 * ã€æ‰©å±•ç¤ºä¾‹ã€‘
 * // åœ¨ models.js ä¸­æ·»åŠ ï¼š
 * function getApiKeyForModel(modelName, customApiKeys = {}) {
 *   const provider = getModelProvider(modelName);
 *   return customApiKeys[provider] || process.env[`${provider.toUpperCase()}_API_KEY`];
 * }
 */
const PROVIDER_CONFIG = {
  openai: {
    baseUrl: "https://api.openai.com/v1",
    apiKeyEnv: "OPENAI_API_KEY", // ç¯å¢ƒå˜é‡åç§°
    headers: (apiKey) => ({
      Authorization: `Bearer ${apiKey}`,
      "Content-Type": "application/json",
    }),
  },
  deepseek: {
    baseUrl: "https://api.deepseek.com/v1",
    apiKeyEnv: "DEEPSEEK_API_KEY", // ç¯å¢ƒå˜é‡åç§°
    headers: (apiKey) => ({
      Authorization: `Bearer ${apiKey}`,
      "Content-Type": "application/json",
    }),
  },
  openrouter: {
    baseUrl: "https://openrouter.ai/api/v1",
    apiKeyEnv: "OPENROUTER_API_KEY", // ç¯å¢ƒå˜é‡åç§°
    headers: (apiKey) => ({
      Authorization: `Bearer ${apiKey}`,
      "Content-Type": "application/json",
      "HTTP-Referer": process.env.OPENROUTER_REFERER || "https://github.com",
      "X-Title": process.env.OPENROUTER_TITLE || "NPC Chat",
    }),
  },
};

/**
 * æ„å»ºæ¶ˆæ¯åˆ—è¡¨
 *
 * ã€åŠŸèƒ½è¯´æ˜ã€‘
 * å°†å†å²äº‹ä»¶è½¬æ¢ä¸º LLM API éœ€è¦çš„æ¶ˆæ¯æ ¼å¼
 *
 * ã€å·¥ä½œæµç¨‹ã€‘
 * 1. éå†å†å²äº‹ä»¶
 * 2. æ ¹æ® fromType è½¬æ¢ä¸º roleï¼ˆuser/assistantï¼‰
 * 3. æå– content
 * 4. æŒ‰æ—¶é—´é¡ºåºæ’åˆ—
 *
 * @param {Array<Object>} events - å†å²äº‹ä»¶åˆ—è¡¨ï¼ˆæŒ‰æ—¶é—´å‡åºï¼‰
 * @returns {Array<Object>} æ¶ˆæ¯åˆ—è¡¨ [{ role: 'user', content: '...' }, ...]
 */
function buildMessages(events) {
  return events.map((event) => {
    const role = event.fromType === "user" ? "user" : "assistant";
    return {
      role,
      content: event.content,
    };
  });
}

/**
 * è°ƒç”¨ LLM API
 *
 * ã€åŠŸèƒ½è¯´æ˜ã€‘
 * è°ƒç”¨æŒ‡å®šæä¾›å•†çš„ LLM API
 *
 * ã€å·¥ä½œæµç¨‹ã€‘
 * 1. è·å–æä¾›å•†é…ç½®
 * 2. è·å– API Key
 * 3. æ„å»ºè¯·æ±‚ URL å’Œ headers
 * 4. å‘é€ POST è¯·æ±‚
 * 5. è§£æå“åº”
 *
 * @param {string} provider - æä¾›å•†åç§°ï¼ˆopenai/deepseek/openrouterï¼‰
 * @param {string} model - æ¨¡å‹åç§°
 * @param {string} systemPrompt - System prompt
 * @param {Array<Object>} messages - æ¶ˆæ¯åˆ—è¡¨
 * @param {number} timeout - è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼Œé»˜è®¤ 30000ï¼‰
 * @returns {Promise<string>} AI å›å¤å†…å®¹
 * @throws {Object} é”™è¯¯å¯¹è±¡ { code, message }
 */
async function callLLMAPI(
  provider,
  model,
  systemPrompt,
  messages,
  timeout = 30000
) {
  const config = PROVIDER_CONFIG[provider];
  if (!config) {
    throw {
      code: "INVALID_PROVIDER",
      message: `ä¸æ”¯æŒçš„æä¾›å•†ï¼š${provider}`,
    };
  }

  // ä»ç¯å¢ƒå˜é‡è¯»å– API Keyï¼ˆæ”¯æŒå¤šä¸ªï¼Œç”¨é€—å·åˆ†éš”ï¼‰
  // æ ¼å¼ï¼šOPENROUTER_API_KEY=key1,key2,key3
  // å¦‚æœæ”¯æŒç”¨æˆ·è‡ªå®šä¹‰ API Keyï¼Œå¯ä»¥æ·»åŠ å‚æ•°ï¼š
  // const apiKeys = customApiKeys || process.env[config.apiKeyEnv];
  const apiKeysStr = process.env[config.apiKeyEnv];
  if (!apiKeysStr) {
    throw {
      code: "API_KEY_MISSING",
      message: `ç¼ºå°‘ ${provider} API Keyï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ ${config.apiKeyEnv}`,
    };
  }

  // è§£æå¤šä¸ª API Keyï¼ˆç”¨é€—å·åˆ†éš”ï¼Œå»é™¤ç©ºæ ¼ï¼‰
  const apiKeys = apiKeysStr
    .split(",")
    .map((key) => key.trim())
    .filter((key) => key.length > 0);

  if (apiKeys.length === 0) {
    throw {
      code: "API_KEY_MISSING",
      message: `ç¼ºå°‘ ${provider} API Keyï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ ${config.apiKeyEnv}`,
    };
  }

  // æ„å»ºè¯·æ±‚ä½“
  const requestBody = {
    model: model,
    messages: [
      {
        role: "system",
        content: systemPrompt,
      },
      ...messages,
    ],
    temperature: 0.7,
  };

  // æŒ‰é¡ºåºå°è¯•æ¯ä¸ª API Keyï¼ˆæ•…éšœè½¬ç§»æœºåˆ¶ï¼‰
  let lastError = null;
  let usedApiKeyIndex = -1;

  console.log(`[LLMService] ğŸ”„ Starting LLM API call with ${apiKeys.length} API Key(s) for provider: ${provider}`);

  for (let i = 0; i < apiKeys.length; i++) {
    const apiKey = apiKeys[i];
    usedApiKeyIndex = i;
    
    console.log(`[LLMService] ğŸ”‘ Trying API Key ${i + 1}/${apiKeys.length}...`);

    // åˆ›å»º AbortController ç”¨äºè¶…æ—¶æ§åˆ¶
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), timeout);

    try {
      const response = await fetch(`${config.baseUrl}/chat/completions`, {
        method: "POST",
        headers: config.headers(apiKey),
        body: JSON.stringify(requestBody),
        signal: controller.signal,
      });

      clearTimeout(timeoutId);

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        const errorStatus = response.status;
        const errorMessage =
          errorData.error?.message ||
          `LLM API è°ƒç”¨å¤±è´¥ï¼š${errorStatus} ${response.statusText}`;

        // åˆ¤æ–­æ˜¯å¦åº”è¯¥å°è¯•ä¸‹ä¸€ä¸ª API Key
        // 401 (Unauthorized), 403 (Forbidden), 429 (Too Many Requests) å¯ä»¥å°è¯•ä¸‹ä¸€ä¸ª
        // 400 (Bad Request), 500+ (Server Error) ä¸åº”è¯¥å°è¯•ä¸‹ä¸€ä¸ªï¼ˆå¯èƒ½æ˜¯è¯·æ±‚æ ¼å¼é—®é¢˜ï¼‰
        const shouldRetryNext =
          errorStatus === 401 ||
          errorStatus === 403 ||
          errorStatus === 429;

        if (shouldRetryNext && i < apiKeys.length - 1) {
          // è®°å½•é”™è¯¯ï¼Œä½†ç»§ç»­å°è¯•ä¸‹ä¸€ä¸ª API Key
          console.warn(
            `[LLMService] âš ï¸  API Key ${i + 1}/${apiKeys.length} failed (${errorStatus}): ${errorMessage}`
          );
          console.warn(
            `[LLMService] ğŸ”„ Trying next API Key (${i + 2}/${apiKeys.length})...`
          );
          lastError = {
            code: "LLM_API_ERROR",
            message: errorMessage,
            status: errorStatus,
            provider,
            apiKeyIndex: i + 1,
          };
          continue; // å°è¯•ä¸‹ä¸€ä¸ª API Key
        }

        // ä¸åº”è¯¥é‡è¯•æˆ–å·²ç»æ˜¯æœ€åä¸€ä¸ª API Keyï¼ŒæŠ›å‡ºé”™è¯¯
        throw {
          code: "LLM_API_ERROR",
          message: errorMessage,
          status: errorStatus,
          provider,
          apiKeyIndex: i + 1,
        };
      }

      const data = await response.json();

      // æå–å›å¤å†…å®¹
      if (
        data.choices &&
        data.choices.length > 0 &&
        data.choices[0].message &&
        data.choices[0].message.content
      ) {
        // æˆåŠŸï¼è®°å½•ä½¿ç”¨çš„ API Key ç´¢å¼•ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        if (i > 0) {
          console.log(
            `[LLMService] âœ… Successfully used API Key ${i + 1}/${apiKeys.length} after ${i} failed attempt(s)`
          );
        } else {
          console.log(`[LLMService] âœ… Successfully used API Key ${i + 1}/${apiKeys.length}`);
        }
        return data.choices[0].message.content.trim();
      }

      throw {
        code: "LLM_API_ERROR",
        message: "LLM API è¿”å›æ ¼å¼å¼‚å¸¸",
        provider,
        apiKeyIndex: i + 1,
      };
    } catch (error) {
      clearTimeout(timeoutId);

      // è¶…æ—¶é”™è¯¯ï¼šå¦‚æœæ˜¯æœ€åä¸€ä¸ª API Keyï¼ŒæŠ›å‡ºé”™è¯¯ï¼›å¦åˆ™å°è¯•ä¸‹ä¸€ä¸ª
      if (error.name === "AbortError") {
        if (i < apiKeys.length - 1) {
          console.warn(
            `[LLMService] API Key ${i + 1}/${apiKeys.length} timeout, trying next...`
          );
          lastError = {
            code: "LLM_API_TIMEOUT",
            message: "LLM API è°ƒç”¨è¶…æ—¶",
            provider,
            apiKeyIndex: i + 1,
          };
          continue;
        }
        throw {
          code: "LLM_API_TIMEOUT",
          message: "æ‰€æœ‰ API Key è°ƒç”¨è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•",
          provider,
        };
      }

      // å¦‚æœæ˜¯æˆ‘ä»¬æŠ›å‡ºçš„é”™è¯¯
      if (error.code) {
        // å¦‚æœæ˜¯å¯é‡è¯•çš„é”™è¯¯ä¸”è¿˜æœ‰æ›´å¤š API Keyï¼Œç»§ç»­å°è¯•
        if (
          (error.code === "LLM_API_ERROR" &&
            (error.status === 401 ||
              error.status === 403 ||
              error.status === 429)) &&
          i < apiKeys.length - 1
        ) {
          lastError = error;
          continue;
        }
        // å¦åˆ™æŠ›å‡ºé”™è¯¯
        throw error;
      }

      // ç½‘ç»œé”™è¯¯æˆ–å…¶ä»–é”™è¯¯ï¼šå¦‚æœæ˜¯æœ€åä¸€ä¸ª API Keyï¼ŒæŠ›å‡ºé”™è¯¯ï¼›å¦åˆ™å°è¯•ä¸‹ä¸€ä¸ª
      const errorMessage = error.message || String(error);
      const errorCause = error.cause
        ? ` (åŸå› : ${error.cause.message || error.cause})`
        : "";

      if (i < apiKeys.length - 1) {
        console.warn(
          `[LLMService] API Key ${i + 1}/${apiKeys.length} network error: ${errorMessage}${errorCause}, trying next...`
        );
        lastError = {
          code: "LLM_API_ERROR",
          message: `LLM API è°ƒç”¨å¤±è´¥ï¼š${errorMessage}${errorCause}`,
          provider,
          originalError: errorMessage,
          errorType: error.name || error.constructor?.name || "Unknown",
          apiKeyIndex: i + 1,
        };
        continue;
      }

      // æœ€åä¸€ä¸ª API Key ä¹Ÿå¤±è´¥äº†ï¼ŒæŠ›å‡ºé”™è¯¯
      throw {
        code: "LLM_API_ERROR",
        message: `æ‰€æœ‰ API Key è°ƒç”¨å¤±è´¥ï¼š${errorMessage}${errorCause}`,
        provider,
        originalError: errorMessage,
        errorType: error.name || error.constructor?.name || "Unknown",
      };
    }
  }

  // æ‰€æœ‰ API Key éƒ½å¤±è´¥äº†ï¼ŒæŠ›å‡ºæœ€åä¸€ä¸ªé”™è¯¯
  if (lastError) {
    console.error(`[LLMService] âŒ All ${apiKeys.length} API Key(s) failed. Last error:`, {
      code: lastError.code,
      message: lastError.message,
      status: lastError.status,
      provider: lastError.provider,
      apiKeyIndex: lastError.apiKeyIndex,
    });
    throw lastError;
  }
  
  // ç†è®ºä¸Šä¸åº”è¯¥åˆ°è¾¾è¿™é‡Œ
  console.error(`[LLMService] âŒ Unexpected error: All API Keys failed but no error recorded`);
  throw {
    code: "LLM_API_ERROR",
    message: "æ‰€æœ‰ API Key è°ƒç”¨å¤±è´¥",
    provider,
  };
}

/**
 * ç”Ÿæˆ AI å›å¤
 *
 * ã€åŠŸèƒ½è¯´æ˜ã€‘
 * è°ƒç”¨ LLM API ç”Ÿæˆå›å¤ï¼ŒåŒ…å«é‡è¯•æœºåˆ¶
 *
 * ã€å·¥ä½œæµç¨‹ã€‘
 * 1. éªŒè¯å‚æ•°
 * 2. è·å–æ¨¡å‹æä¾›å•†
 * 3. æ„å»ºæ¶ˆæ¯åˆ—è¡¨
 * 4. è°ƒç”¨ LLM APIï¼ˆå¸¦é‡è¯•ï¼‰
 * 5. è¿”å›å›å¤å†…å®¹
 *
 * ã€é‡è¯•æœºåˆ¶ã€‘
 * - æœ€å¤šé‡è¯• 2 æ¬¡ï¼ˆæ€»å…± 3 æ¬¡å°è¯•ï¼‰
 * - é‡è¯•é—´éš”ï¼š1 ç§’ï¼ˆæŒ‡æ•°é€€é¿ï¼š1s, 2sï¼‰
 * - è¶…æ—¶é”™è¯¯ä¸é‡è¯•
 * - API Key é”™è¯¯ä¸é‡è¯•
 *
 * @param {Object} options - è°ƒç”¨é€‰é¡¹
 * @param {string} options.model - æ¨¡å‹åç§°
 * @param {string} [options.provider] - æä¾›å•†åç§°ï¼ˆå¯é€‰ï¼Œé¢„è®¾æ¨¡å‹ä¼šè‡ªåŠ¨æ¨æ–­ï¼‰
 * @param {string} options.systemPrompt - System prompt
 * @param {Array<Object>} options.messages - æ¶ˆæ¯åˆ—è¡¨ï¼ˆå†å²äº‹ä»¶ï¼‰
 * @param {number} [options.timeout] - è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼Œé»˜è®¤ 30000ï¼‰
 * @returns {Promise<string>} AI å›å¤å†…å®¹
 * @throws {Object} é”™è¯¯å¯¹è±¡ { code, message }
 */
async function generateReply(options) {
  const { model, provider, systemPrompt, messages, timeout = 30000 } = options;

  // å‚æ•°éªŒè¯
  if (!model || typeof model !== "string") {
    throw {
      code: "VALIDATION_ERROR",
      message: "æ¨¡å‹åç§°ä¸èƒ½ä¸ºç©º",
    };
  }

  if (!systemPrompt || typeof systemPrompt !== "string") {
    throw {
      code: "VALIDATION_ERROR",
      message: "System prompt ä¸èƒ½ä¸ºç©º",
    };
  }

  if (!Array.isArray(messages)) {
    throw {
      code: "VALIDATION_ERROR",
      message: "æ¶ˆæ¯åˆ—è¡¨å¿…é¡»æ˜¯æ•°ç»„",
    };
  }

  // è·å–æ¨¡å‹æä¾›å•†
  // ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„ providerï¼ˆå¦‚æœæä¾›ï¼‰ï¼Œå¦åˆ™ä»é¢„è®¾æ¨¡å‹æ¨æ–­
  let modelProvider = provider;
  if (!modelProvider) {
    modelProvider = getModelProvider(model);
  }

  // å¦‚æœä»ç„¶æ²¡æœ‰ providerï¼Œè¯´æ˜æ˜¯è‡ªå®šä¹‰æ¨¡å‹ä½†æ²¡æœ‰æŒ‡å®šæä¾›å•†
  if (!modelProvider) {
    throw {
      code: "PROVIDER_REQUIRED",
      message: `è‡ªå®šä¹‰æ¨¡å‹ ${model} éœ€è¦æŒ‡å®šæä¾›å•†ï¼ˆproviderï¼‰`,
    };
  }

  // éªŒè¯æä¾›å•†æ˜¯å¦å¯ç”¨
  const { isProviderEnabled } = require("../config/models");
  if (!isProviderEnabled(modelProvider)) {
    throw {
      code: "INVALID_PROVIDER",
      message: `æä¾›å•† ${modelProvider} æœªå¯ç”¨ï¼Œè¯·åœ¨ç¯å¢ƒå˜é‡ä¸­å¯ç”¨`,
    };
  }

  // æ„å»ºæ¶ˆæ¯åˆ—è¡¨ï¼ˆå¦‚æœä¼ å…¥çš„æ˜¯äº‹ä»¶åˆ—è¡¨ï¼Œéœ€è¦è½¬æ¢ï¼‰
  let formattedMessages = messages;
  // åˆ¤æ–­æ˜¯å¦æ˜¯äº‹ä»¶åˆ—è¡¨ï¼šæ£€æŸ¥ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯å¦æœ‰ fromType å­—æ®µ
  if (
    messages.length > 0 &&
    typeof messages[0] === "object" &&
    messages[0].fromType !== undefined
  ) {
    // å¦‚æœä¼ å…¥çš„æ˜¯äº‹ä»¶åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºæ¶ˆæ¯æ ¼å¼
    formattedMessages = buildMessages(messages);
  }

  // é‡è¯•æœºåˆ¶
  const maxRetries = 2;
  const retryDelay = 1000; // 1 ç§’

  let lastError = null;

  for (let attempt = 0; attempt <= maxRetries; attempt++) {
    try {
      const reply = await callLLMAPI(
        modelProvider,
        model,
        systemPrompt,
        formattedMessages,
        timeout
      );
      return reply;
    } catch (error) {
      lastError = error;

      // ä¸é‡è¯•çš„é”™è¯¯ï¼šè¶…æ—¶ã€API Key é”™è¯¯ã€æ— æ•ˆæ¨¡å‹
      if (
        error.code === "LLM_API_TIMEOUT" ||
        error.code === "API_KEY_MISSING" ||
        error.code === "INVALID_MODEL" ||
        error.code === "INVALID_PROVIDER"
      ) {
        throw error;
      }

      // æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼ŒæŠ›å‡ºé”™è¯¯
      if (attempt === maxRetries) {
        throw error;
      }

      // ç­‰å¾…åé‡è¯•ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
      const delay = retryDelay * (attempt + 1);
      await new Promise((resolve) => setTimeout(resolve, delay));
    }
  }

  // ç†è®ºä¸Šä¸ä¼šæ‰§è¡Œåˆ°è¿™é‡Œ
  throw (
    lastError || {
      code: "LLM_API_ERROR",
      message: "LLM API è°ƒç”¨å¤±è´¥",
    }
  );
}

/**
 * ä»äº‹ä»¶åˆ—è¡¨æ„å»º Prompt
 *
 * ã€åŠŸèƒ½è¯´æ˜ã€‘
 * å°†å†å²äº‹ä»¶è½¬æ¢ä¸º LLM API éœ€è¦çš„æ ¼å¼
 *
 * ã€å·¥ä½œæµç¨‹ã€‘
 * 1. éªŒè¯äº‹ä»¶åˆ—è¡¨
 * 2. è½¬æ¢ä¸ºæ¶ˆæ¯æ ¼å¼
 * 3. è¿”å›æ¶ˆæ¯åˆ—è¡¨
 *
 * @param {Array<Object>} events - å†å²äº‹ä»¶åˆ—è¡¨ï¼ˆæŒ‰æ—¶é—´å‡åºï¼‰
 * @returns {Array<Object>} æ¶ˆæ¯åˆ—è¡¨
 */
function buildPromptFromEvents(events) {
  if (!Array.isArray(events)) {
    return [];
  }

  return buildMessages(events);
}

module.exports = {
  generateReply,
  buildPromptFromEvents,
};
